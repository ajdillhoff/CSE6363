{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoTokenizer, AutoModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sinusoidal Positional Encoding\n",
    "\n",
    "The positional encoding function used in the original Transformer paper is\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "PE_{pos, 2i} &= \\sin\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right)\\\\\n",
    "PE_{pos, 2i+1} &= \\cos\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "This was chosen based on the following desirable properties:\n",
    "\n",
    "1. The positional encodings are unique for different positions.\n",
    "2. The encoding function is continuous and differentiable.\n",
    "3. Encodings are linear with respect to the position.\n",
    "4. The encodings should generalize to out-of-training sequence lengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sinusoidal_pos_encoding(n_position, dim):\n",
    "    def cal_angle(position, hid_idx):\n",
    "        return position / np.power(10000, 2 * (hid_idx // 2) / dim)\n",
    "    def get_posi_angle_vec(position):\n",
    "        return [cal_angle(position, hid_j) for hid_j in range(dim)]\n",
    "    sinusoidal_encoding = torch.tensor([get_posi_angle_vec(pos_i) for pos_i in range(n_position)])\n",
    "    sinusoidal_encoding[:, 0::2] = torch.sin(sinusoidal_encoding[:, 0::2])  # dim 2i\n",
    "    sinusoidal_encoding[:, 1::2] = torch.cos(sinusoidal_encoding[:, 1::2])  # dim 2i+1\n",
    "    return sinusoidal_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"Qwen/Qwen2.5-0.5B\"\n",
    "tok = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModel.from_pretrained(model_id)\n",
    "\n",
    "sequence1 = \"Naomi went to the store.\"\n",
    "sequence2 = \"Naomi went to the store to buy some reaction mass pellets.\"\n",
    "tokens1 = tok(sequence1, return_tensors=\"pt\")[\"input_ids\"]\n",
    "embeddings1 = model.embed_tokens(tokens1)\n",
    "tokens2 = tok(sequence2, return_tensors=\"pt\")[\"input_ids\"]\n",
    "embeddings2 = model.embed_tokens(tokens2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokens1.shape, embeddings1.shape)\n",
    "print(tokens2.shape, embeddings2.shape)\n",
    "\n",
    "# Generate position encodings for each sequence\n",
    "pos_enc1 = sinusoidal_pos_encoding(tokens1.shape[1], model.config.hidden_size)\n",
    "pos_enc2 = sinusoidal_pos_encoding(tokens2.shape[1], model.config.hidden_size)\n",
    "\n",
    "print(pos_enc1.shape, pos_enc2.shape)\n",
    "\n",
    "# compare the positional encodings beteween the two sequences\n",
    "for i in range(pos_enc1.shape[0]):\n",
    "    print(f\"pos {i} same: \", torch.allclose(pos_enc1[i], pos_enc2[i]))\n",
    "\n",
    "# show distances beween i and i+1 for each encoding for the first 7 positions\n",
    "print(\"Distances between consecutive positions for encoding 1\")\n",
    "for i in range(6):\n",
    "    print(f\"pos {i} diff: \", torch.dist(pos_enc1[i], pos_enc1[i+1]))\n",
    "\n",
    "# show distances beween i and i+1 for each encoding for the first 7 positions\n",
    "print(\"Distances between consecutive positions for encoding 2\")\n",
    "for i in range(6):\n",
    "    print(f\"pos {i} diff: \", torch.dist(pos_enc2[i], pos_enc2[i+1]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
