{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_decision_boundary(weights):\n",
    "    m = -weights[2] / weights[1]\n",
    "    b = -weights[0]\n",
    "    return np.array([m, b])\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1.0 / (1.0 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent\n",
    "\n",
    "Let's start with an intuitive explanation. You're lost in the mountains with zero visibility and need to get back to the bottom. At each step, you can feel the grade of the slope beneath your feet. If the slope is descending, you continue forward, otherwise you change course.\n",
    "\n",
    "We can visualize this with a simple mathematical sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-1, 1, 100)\n",
    "y = x**2\n",
    "\n",
    "# loc depicts our current position\n",
    "loc_x = 1.0\n",
    "loc_y = loc_x**2\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot(x, y)\n",
    "ax.scatter(loc_x, loc_y, c='r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are at the top of the hill. How do determine the slope of a function $f$ at a given point $x$?\n",
    "\n",
    "Good old calculus: $\\frac{df}{dx}$. In this case $\\frac{df}{dx} = 2x$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loc depicts our current position\n",
    "d_loc = 2 * loc_x\n",
    "print(\"Derivative at x = {} is {}\".format(loc_x, d_loc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have calculated that a x = 1, the slope (derivative) of the function is 2. Now to use this knowledge to step forward.\n",
    "\n",
    "## Update Step\n",
    "\n",
    "The all important update step is actually quite simple:\n",
    "\n",
    "$$x_{n+1} = x_n - \\frac{df}{dx}.$$\n",
    "\n",
    "Let's apply this update step and check our position again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_new = loc_x - d_loc\n",
    "y_new = x_new ** 2\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot(x, y)\n",
    "ax.scatter(loc_x, loc_y, c='g')\n",
    "ax.scatter(x_new, y_new, c='r')\n",
    "ax.set_title(\"One Step of Gradient Descent\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow we took a really big step and ended up on the other side of the function. It seems we missed an important part of gradient descent: the **step size**. To control the step size, we add a single parameter to the update step:\n",
    "\n",
    "$$x_{n+1} = x_n - \\alpha * \\frac{df}{dx}.$$\n",
    "\n",
    "This new value $\\alpha$ is a parameter that controls how big of a step we can take. In the context of machine learning, the step size is called the **learning rate**. Let's reduce this to 0.1 and see where it takes us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.1\n",
    "x_new = loc_x - alpha * d_loc\n",
    "y_new = x_new ** 2\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot(x, y)\n",
    "ax.scatter(x_new, y_new, c='r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's use gradient descent to optimize our original linear classifier. Again, here is the data generated from our two distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_samples = np.random.multivariate_normal([-1, 1], [[0.1, 0], [0, 0.1]], 100)\n",
    "b_samples = np.random.multivariate_normal([1, -1], [[0.1, 0], [0, 0.1]], 100)\n",
    "a_targets = np.zeros(100)  # Samples from class A are assigned a class value of 0.\n",
    "b_targets = np.ones(100)  # Samples from class B are assigned a class value of 1.\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.scatter(a_samples[:, 0], a_samples[:, 1], c='b')\n",
    "ax.scatter(b_samples[:, 0], b_samples[:, 1], c='r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To set this up, we will **initialize** our network's parameters to be random values. We'll also introduce another neat trick. By including the bias in our parameter list, we can slightly simplify the forward calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classifier Parameters\n",
    "weights = np.random.rand(2) \n",
    "bias = 1\n",
    "weights = np.concatenate(([bias], weights))\n",
    "\n",
    "# For visualizing the line\n",
    "m, b = calc_decision_boundary(weights)\n",
    "\n",
    "# If the slope is undefined, it is vertical.\n",
    "if weights[1] != 0:\n",
    "    x = np.linspace(-1, 1, 100)\n",
    "    y = m * x + b\n",
    "else:\n",
    "    x = np.zeros(100) + b\n",
    "    y = np.linspace(-1, 1, 100)\n",
    "    \n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot(x, y, c='g')\n",
    "ax.scatter(a_samples[:, 0], a_samples[:, 1], c='b')\n",
    "ax.scatter(b_samples[:, 0], b_samples[:, 1], c='r')\n",
    "plt.axis('equal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the bias is included in our weights vector, how do we compute the result? Concatenate a 1 to the input before multiplying."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear combination of weights and input\n",
    "y_a = weights @ np.concatenate((np.ones((100, 1)), a_samples), axis=-1).T\n",
    "y_b = weights @ np.concatenate((np.ones((100, 1)), b_samples), axis=-1).T\n",
    "\n",
    "# Sigmoid function\n",
    "pred_a = sigmoid(y_a)\n",
    "pred_b = sigmoid(y_b)\n",
    "\n",
    "l2_a = 0.5 * ((a_targets - pred_a)**2)\n",
    "l2_b = 0.5 * ((b_targets - pred_b)**2)\n",
    "loss_a = l2_a.sum()\n",
    "loss_b = l2_b.sum()\n",
    "print(\"Loss A = {}\".format(loss_a))\n",
    "print(\"Loss B = {}\".format(loss_b))\n",
    "\n",
    "# Combine and normalize the error between 0 and 1.\n",
    "loss = np.concatenate((l2_a, l2_b)).mean()\n",
    "print(\"Normalized loss = {}\".format(loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So how do we use gradient descent to optimize our simple perceptron? First, note that we will be using the sigmoid function because it is **continuous**. This is a very important property when it comes to optimization because derivatives on discrete functions are undefined, although they can be approximated.\n",
    "\n",
    "Our current classifier is a series of two functions followed by a loss function:\n",
    "\n",
    "$$L(g(\\mathbf{w}\\mathbf{x}))$$\n",
    "\n",
    "where $g(x) = \\frac{1}{1 + \\exp^{-x}}$\n",
    "\n",
    "With each update of gradient descent we want to modify our parameters $\\mathbf{w}$. So we need to calculate $\\frac{\\partial L}{\\partial w_i}$. To find this gradient, we utilize the chain rule from Calculus which is:\n",
    "\n",
    "$$\\frac{dz}{dx} = \\frac{dz}{dy} \\cdot \\frac{dy}{dx}.$$\n",
    "\n",
    "Applying this to our classifier, we have:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial w_i} = \\frac{\\partial L}{\\partial y} \\frac{\\partial y}{\\partial a} \\frac{\\partial a}{\\partial w_i}$$\n",
    "\n",
    "Here we let $a = \\sum_i w_i x_i$ so $\\frac{\\partial a}{\\partial w_i} = x_i$ and $\\frac{dy}{da} = g'(a) = g(a) * (1 - g(a))$.\n",
    "\n",
    "The squared L2 loss function we are using is again defined as:\n",
    "\n",
    "$$L = \\frac{1}{2} (\\hat{y} - y)^2$$\n",
    "\n",
    "This is a convenient choice because the derivative is simple to calculate:\n",
    "\n",
    "$$\\frac{dL}{dy} = \\hat{y} - y.$$\n",
    "\n",
    "Written fully,\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial w_i} = (\\hat{y} - y) * g(a) * (1 - g(a)) * x_i.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine our dataset into one single object\n",
    "samples = np.concatenate((a_samples, b_samples))\n",
    "targets = np.concatenate((a_targets, b_targets))\n",
    "print(samples.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.random.randint(200, size=1)\n",
    "alpha = 1\n",
    "x = samples[idx]\n",
    "y_hat = targets[idx]\n",
    "print(x.shape)\n",
    "\n",
    "# Linear combination of weights and input\n",
    "a = weights @ np.concatenate((np.ones((1, 1)), x), axis=-1).T\n",
    "\n",
    "# Sigmoid function\n",
    "z = sigmoid(a)\n",
    "\n",
    "loss = 0.5 * (z - y_hat)**2\n",
    "print(\"Loss = {}\".format(loss))\n",
    "\n",
    "dw0 = (z - y_hat) * z * (1 - z)\n",
    "dw1 = (z - y_hat) * z * (1 - z) * x[0, 0]\n",
    "dw2 = (z - y_hat) * z * (1 - z) * x[0, 1]\n",
    "print(dw0, dw1)\n",
    "weights[0] = weights[0] - alpha * dw0\n",
    "weights[1] = weights[1] - alpha * dw1\n",
    "weights[2] = weights[2] - alpha * dw2\n",
    "\n",
    "# for i in range(200):\n",
    "#     idx = np.random.randint(200, size=1)\n",
    "#     alpha = 1\n",
    "#     x = samples[idx]\n",
    "#     y_hat = targets[idx]\n",
    "\n",
    "#     # Linear combination of weights and input\n",
    "#     a = weights @ np.concatenate((np.ones((1, 1)), x), axis=-1).T\n",
    "\n",
    "#     # Sigmoid function\n",
    "#     z = sigmoid(a)\n",
    "\n",
    "#     loss = 0.5 * (z - y_hat)**2\n",
    "\n",
    "#     dw0 = (z - y_hat) * z * (1 - z)\n",
    "#     dw1 = (z - y_hat) * z * (1 - z) * x[0, 0]\n",
    "#     dw2 = (z - y_hat) * z * (1 - z) * x[0, 1]\n",
    "#     weights[0] = weights[0] - alpha * dw0\n",
    "#     weights[1] = weights[1] - alpha * dw1\n",
    "#     weights[2] = weights[2] - alpha * dw2\n",
    "\n",
    "# For visualizing the line\n",
    "m, b = calc_decision_boundary(weights)\n",
    "\n",
    "# If the slope is undefined, it is vertical.\n",
    "if weights[1] != 0:\n",
    "    x = np.linspace(-1, 1, 100)\n",
    "    y = m * x + b\n",
    "else:\n",
    "    x = np.zeros(100) + b\n",
    "    y = np.linspace(-1, 1, 100)\n",
    "    \n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "# Plot decision boundary\n",
    "ax.axline((0, b), slope=m, c='g')\n",
    "ax.scatter(a_samples[:, 0], a_samples[:, 1], c='b')\n",
    "ax.scatter(b_samples[:, 0], b_samples[:, 1], c='r')\n",
    "plt.axis('equal')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
