{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation\n",
    "\n",
    "This notebook demonstrates the backpropagation algorithm to update the weights of a neural network. The derived result is compared against PyTorch, a popular machine learning framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Simple Regression Problem\n",
    "\n",
    "Given a dataset made up of 8 points sampled along the line $y = x$, our goal is to fit a neural network with 1 hidden layer consisting of a single node followed by an output layer with a single node. We will use mean-squared error as our error function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.linspace(-1, 1, 8)\n",
    "targets = data\n",
    "\n",
    "# Create a simple network in Pytorch\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "        self.hidden = nn.Linear(1, 1)\n",
    "        self.activation = nn.Sigmoid()\n",
    "        self.output = nn.Linear(1, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        a1 = self.hidden(x)\n",
    "        z1 = self.activation(a1)\n",
    "        a2 = self.output(z1)\n",
    "\n",
    "        return [a1, z1, a2]\n",
    "\n",
    "model = MyModel()\n",
    "\n",
    "weights_dict = OrderedDict(\n",
    "    {\n",
    "        'hidden.weight': torch.tensor([[-0.1]]),\n",
    "        'hidden.bias': torch.tensor([0]),\n",
    "        'output.weight': torch.tensor([[-0.1]]),\n",
    "        'output.bias': torch.tensor([0])\n",
    "    }\n",
    ")\n",
    "\n",
    "model.load_state_dict(weights_dict)\n",
    "\n",
    "loss_fn = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pt = torch.from_numpy(data.astype(np.float32))\n",
    "targets_pt = torch.from_numpy(targets.astype(np.float32))\n",
    "[a1, z1, y_hat] = model(data_pt.unsqueeze(1))\n",
    "print(y_hat)\n",
    "\n",
    "loss = loss_fn(y_hat, targets_pt.unsqueeze(1))\n",
    "loss.retain_grad()\n",
    "print(loss)\n",
    "\n",
    "# Compute the backward pass\n",
    "model.zero_grad()\n",
    "loss.backward()\n",
    "\n",
    "print(f'** PyTorch Gradients **')\n",
    "print(f'Loss = {loss.grad}')\n",
    "print(f'Hidden Weight = {model.hidden.weight.grad.item():.3}')\n",
    "print(f'Hidden Bias = {model.hidden.bias.grad.item():.3}')\n",
    "print(f'Output Weight = {model.output.weight.grad.item():.3}')\n",
    "print(f'Output Bias = {model.output.bias.grad.item():.3}')\n",
    "\n",
    "# For computing gradients manually\n",
    "print(f'\\n** Manual Computation **')\n",
    "dLdy_hat = 2 * (y_hat - targets_pt.unsqueeze(1))\n",
    "dy_hatdw2 = dLdy_hat * z1\n",
    "dy_hatdb2 = dLdy_hat * 1\n",
    "dy_hatdz1 = model.hidden.weight.item()\n",
    "\n",
    "# Computing back to layer 1\n",
    "dz1da1 = z1 * (1 - z1) # sigmoid derivative\n",
    "da1dw = data_pt.unsqueeze(1)\n",
    "da1db = 1\n",
    "\n",
    "# Combining Results\n",
    "dy_hatdw1 = dLdy_hat * dy_hatdz1 * dz1da1 * da1dw\n",
    "dy_hatdb1 = dLdy_hat * dy_hatdz1 * dz1da1 * da1db\n",
    "\n",
    "print(f'Hidden Weight = {dy_hatdw1.mean().item():.3}')\n",
    "print(f'Hidden Bias = {dy_hatdb1.mean().item():.3}')\n",
    "print(f'Output Weight = {dy_hatdw2.mean().item():.3}')\n",
    "print(f'Output Bias = {dy_hatdb2.mean().item():.3}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing the Gradients Manually\n",
    "\n",
    "Now let's compute the gradients of each function.\n",
    "If we do this correctly, the result will match the output of PyTorch.\n",
    "\n",
    "## The Loss Function\n",
    "\n",
    "The loss function we are using is Mean Squared Error:\n",
    "\n",
    "$$\n",
    "\\mathcal{L} = \\text{MSE}(\\hat{\\mathbf{y}}, \\mathbf{y}) = \\frac{1}{n} \\sum_{i=1}^{n} (\\hat{y}_i - y_i)^2\n",
    "$$\n",
    "\n",
    "In the example above, our `loss` is `0.4289`. This is the value that is produced by $\\mathcal{L}$.\n",
    "The first gradient is\n",
    "\n",
    "$$\n",
    "\\frac{\\partial E}{\\partial E} = 1,\n",
    "$$\n",
    "\n",
    "confirmed by calling `loss.grad` in the code above.\n",
    "\n",
    "## Output Layer\n",
    "\n",
    "There are two inputs into $\\mathcal{L}$: \n",
    "1. The output of our output layer $\\hat{\\mathbf{y}}$.\n",
    "2. The targets $\\mathbf{y}$.\n",
    "\n",
    "There are two gradients we could compute then. The gradient with respect to the targets $\\mathbf{y}$ are not useful here, so we will only consider\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{\\hat{y}}} = \\mathbf{\\hat{y}} - \\mathbf{y}\n",
    "$$\n",
    "\n",
    "To get the gradients with respect to the weights and bias, we compute\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial \\mathbf{\\hat{y}}}{\\partial \\mathbf{w}} &= \\mathbf{z}^{(1)}\\\\[0.5em]\n",
    "\\frac{\\partial \\mathbf{\\hat{y}}}{\\partial \\mathbf{b}} &= 1\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "We now have the values needed to compute the gradients for the output layer.\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{w}^{(2)}} &= \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{\\hat{y}}} \\frac{\\partial \\mathbf{\\hat{y}}}{\\partial \\mathbf{w}^{(2)}} = \\\\\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{w}^{(2)}} &= \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{\\hat{y}}} \\frac{\\partial \\mathbf{\\hat{y}}}{\\partial \\mathbf{w}^{(2)}}\\\\\n",
    "\\end{align*}\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cse6363",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
