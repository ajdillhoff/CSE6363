{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24a3e817-c112-4d12-8426-49647f96a62f",
   "metadata": {},
   "source": [
    "# Neural Networks\n",
    "\n",
    "This notebook implements a simple neural network for multi-class classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ab5b50-0cf9-48d6-bd47-ef0a74d98e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy.random import MT19937\n",
    "from numpy.random import RandomState, SeedSequence\n",
    "from sklearn.datasets import fetch_openml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0234cf0-97f5-4ff3-8194-ed5673c35e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "data, targets = fetch_openml(\"mnist_784\", version=1, return_X_y=True, as_frame=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaae3869",
   "metadata": {},
   "source": [
    "# Normalization vs. Standardization\n",
    "\n",
    "Scaling features in general is useful for many machine learning algorithms since most of them are not scale-invariant.\n",
    "\n",
    "Normalization bounds the features to a fixed interval. This is typically done to scale features to $[0, 1]$.\n",
    "Standardization centers the features with mean 0 and standard deviation 1. This results in features that have the same parameters as a standard normal distribution.\n",
    "\n",
    "Standardization is well suited for optimization algorithms such as gradient descent where the weights are typically randomly generated with small values centered around 0. Additionally, outliers are well preserved with standardization as opposed to normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa21950-83c7-46e5-a16b-a2fc7fa0df6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "X_train = data[:60000]\n",
    "y_train = targets[:60000]\n",
    "X_test = data[60000:]\n",
    "y_test = targets[60000:]\n",
    "\n",
    "# Normalization\n",
    "# X_train = X_train / 255.\n",
    "# X_test = X_test / 255.\n",
    "\n",
    "# Standardization\n",
    "X_train = (X_train - X_train.mean()) / X_train.std()\n",
    "X_test = (X_test - X_test.mean()) / X_test.std() # Is this correct?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6634f8c-d26f-49a9-a8c1-bec5c09f7987",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_one_hot(targets, nb_classes):\n",
    "    res = np.eye(nb_classes)[np.array(targets).reshape(-1)]\n",
    "    return res.reshape(list(targets.shape)+[nb_classes])\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1. / (1. + np.exp(-x))\n",
    "\n",
    "\n",
    "def sigmoid_grad(out, grad_in):\n",
    "    return out * (1 - out) * grad_in\n",
    "\n",
    "\n",
    "def softmax(x):\n",
    "    num = np.exp(x - np.max(x))\n",
    "    \n",
    "    return num / np.sum(num, axis=0, keepdims=True)\n",
    "\n",
    "\n",
    "def mse(x, y):\n",
    "    return np.mean((x - y)**2)\n",
    "\n",
    "\n",
    "def mse_grade(x, y):\n",
    "    return x - y\n",
    "\n",
    "\n",
    "# def softmax_grad(out, grad_in):\n",
    "#       return (np.einsum('ij,jk->ijk', out, np.eye(out.shape[-1])) \\\n",
    "#            - np.einsum('ij,ik->ijk', out, out))\n",
    "    \n",
    "    \n",
    "def softmax_grad(probs, bp_err):\n",
    "    dim = probs.shape[1]\n",
    "    output = np.empty(probs.shape)\n",
    "    for j in range(dim):\n",
    "        d_prob_over_xj = - (probs * probs[:,[j]])  # i.e. prob_k * prob_j, no matter k==j or not\n",
    "        d_prob_over_xj[:,j] += probs[:,j]   # i.e. when k==j, +prob_j\n",
    "        output[:,j] = np.sum(bp_err * d_prob_over_xj, axis=1)\n",
    "    return output\n",
    "\n",
    "\n",
    "def cross_entropy_loss(pred, target):\n",
    "    return -target * np.log(pred)\n",
    "\n",
    "\n",
    "def cross_entropy_grad(pred, target):\n",
    "    return target - pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2601a58a-b102-45d6-a4c8-71bb2769455d",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = X_train.shape[1]\n",
    "num_classes = 10\n",
    "\n",
    "# Output layer configuration\n",
    "rs = RandomState(MT19937(SeedSequence(42))) # for consistency\n",
    "layer1_weights = (rs.rand(num_classes, num_features + 1) - 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc9c4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the first sample with shape (1, 784)\n",
    "sample = X_test[0].reshape(1, -1)\n",
    "\n",
    "# Forward pass\n",
    "layer1_a = layer1_weights @ np.concatenate((np.ones((sample.shape[0], 1)), sample), axis=1).T\n",
    "layer1_z = softmax(layer1_a)\n",
    "loss = cross_entropy_loss(layer1_z.T, get_one_hot(y_test.astype(int)[0].reshape(1, -1), num_classes))\n",
    "print(np.mean(loss, axis=0))\n",
    "\n",
    "# Backward pass\n",
    "d_loss = cross_entropy_grad(layer1_z.T, get_one_hot(y_test.astype(int)[0].reshape(1, -1), num_classes))\n",
    "d_layer1_weights = d_loss.T @ np.concatenate((np.ones((sample.shape[0], 1)), sample), axis=1)\n",
    "d_layer1_weights = d_layer1_weights.squeeze(1)\n",
    "print(d_layer1_weights.shape)\n",
    "\n",
    "# Update weights\n",
    "layer1_weights += 0.01 * d_layer1_weights\n",
    "print(layer1_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "775f963f",
   "metadata": {},
   "source": [
    "# Batch Training\n",
    "\n",
    "The cell below performs batch training over an entire epoch of 60k samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95138b10-ffa3-488c-b7f2-cc3e038252fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X, y):\n",
    "    # Initialize weights\n",
    "    rs = RandomState(MT19937(SeedSequence(42))) # for consistency\n",
    "    layer1_weights = (rs.rand(num_classes, num_features + 1) - 0.5)\n",
    "\n",
    "    losses = []\n",
    "    batch_size = 32\n",
    "    correct = 0\n",
    "    lr = 0.01\n",
    "\n",
    "    for batch_idx in range(0, X.shape[0], batch_size):\n",
    "        start_idx = batch_idx\n",
    "        end_idx = start_idx + batch_size if start_idx + batch_size < X.shape[0] else X.shape[0]\n",
    "        batch_samples = X[start_idx : end_idx]\n",
    "        batch_targets = get_one_hot(y[start_idx : end_idx].astype(int), 10)\n",
    "\n",
    "        # Forward Pass\n",
    "        layer1_a = layer1_weights @ np.concatenate((np.ones((batch_samples.shape[0], 1)), batch_samples), axis=1).T\n",
    "        layer1_z = softmax(layer1_a)\n",
    "\n",
    "        # print(layer1_a, layer1_z)\n",
    "\n",
    "        loss = cross_entropy_loss(layer1_z.T, batch_targets)\n",
    "\n",
    "        # print(loss)\n",
    "        loss = np.sum(np.mean(loss, axis=0))\n",
    "        losses.append(loss)\n",
    "\n",
    "        # Update the number of correct training predictions\n",
    "        correct += np.sum(np.argmax(layer1_z, axis=0) == y[start_idx : end_idx].astype(int))\n",
    "\n",
    "        # Backward Pass\n",
    "        d_loss = cross_entropy_grad(layer1_z.T, batch_targets)\n",
    "        # print(d_loss)\n",
    "        # d_layer1_z = softmax_grad(layer1_z.T, d_loss) # Not necessary! `cross_entropy_grad` includes the softmax derivative.\n",
    "        d_layer1_weights = d_loss.T @ np.concatenate((np.ones((batch_samples.shape[0], 1)), batch_samples), axis=1)\n",
    "        # print(d_layer1_weights)\n",
    "\n",
    "        # Gradient step\n",
    "        layer1_weights = layer1_weights + lr * d_layer1_weights\n",
    "\n",
    "    return correct, losses, layer1_weights\n",
    "\n",
    "def test(X, y, weights):\n",
    "    batch_size = 32\n",
    "    correct = 0\n",
    "\n",
    "    for batch_idx in range(0, X.shape[0], batch_size):\n",
    "        start_idx = batch_idx\n",
    "        end_idx = start_idx + batch_size if start_idx + batch_size < X.shape[0] else X.shape[0]\n",
    "        batch_samples = X[start_idx : end_idx]\n",
    "\n",
    "        # Forward Pass\n",
    "        layer1_a = weights @ np.concatenate((np.ones((batch_samples.shape[0], 1)), batch_samples), axis=1).T\n",
    "        layer1_z = softmax(layer1_a)\n",
    "\n",
    "        # Update the number of correct training predictions\n",
    "        correct += np.sum(np.argmax(layer1_z, axis=0) == y[start_idx : end_idx].astype(int))\n",
    "\n",
    "    return correct\n",
    "\n",
    "train_correct, losses, weights = train(X_train, y_train)\n",
    "test_correct = test(X_test, y_test, weights)\n",
    "\n",
    "print(f'Training Accuracy: {train_correct / X_train.shape[0] * 100:.2f}%')\n",
    "print(f'Testing Accuracy: {test_correct / X_test.shape[0] * 100:.2f}%')\n",
    "\n",
    "# Plot loss curve\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot()\n",
    "ax.plot(losses)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cse6363",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "d3b6a6ff37777e29af01f3b2e09adff2d731c41e68df337dd356cdd606497958"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
