{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a6ea01-88b2-440a-994a-d55135ce98a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_circles\n",
    "from sklearn.model_selection import train_test_split\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X, y = make_circles(n_samples=1_000, factor=0.3, noise=0.05, random_state=0)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=0)\n",
    "\n",
    "_, (train_ax, test_ax) = plt.subplots(ncols=2, sharex=True, sharey=True, figsize=(8, 4))\n",
    "\n",
    "train_ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train)\n",
    "train_ax.set_ylabel(\"Feature #1\")\n",
    "train_ax.set_xlabel(\"Feature #0\")\n",
    "train_ax.set_title(\"Training data\")\n",
    "\n",
    "test_ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test)\n",
    "test_ax.set_xlabel(\"Feature #0\")\n",
    "_ = test_ax.set_title(\"Testing data\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a1c89322-847d-407b-8444-c42c7e1a0014",
   "metadata": {},
   "source": [
    "As it is, the data is not linearly separable.\n",
    "We could come up with a fairly simple transformation into 3D resulting in a linearly separable datset.\n",
    "\n",
    "Let $\\phi(\\mathbf{x}) = (x_1^2, x_2^2, \\sqrt{2}x_1x_2)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dbe5062-8bf7-4393-afba-53fe7d8a26f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_transform(x):\n",
    "    return np.vstack((x[:, 0]**2, x[:, 1]**2, np.sqrt(2) * x[:, 0] * x[:, 1])).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d564ee99-5bd5-4a52-b204-066f2a564d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_k = feature_transform(X_train)\n",
    "X_test_k = feature_transform(X_test)\n",
    "\n",
    "_, (train_ax, test_ax) = plt.subplots(ncols=2, sharex=True, sharey=True, figsize=(8, 4), subplot_kw=dict(projection='3d'))\n",
    "\n",
    "train_ax.scatter(X_train_k[:, 0], X_train_k[:, 1], X_train_k[:, 2], c=y_train)\n",
    "train_ax.set_ylabel(\"Feature #1\")\n",
    "train_ax.set_xlabel(\"Feature #0\")\n",
    "train_ax.set_title(\"Training data\")\n",
    "\n",
    "test_ax.scatter(X_test_k[:, 0], X_test_k[:, 1], c=y_test)\n",
    "test_ax.set_xlabel(\"Feature #0\")\n",
    "_ = test_ax.set_title(\"Testing data\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5c43efb8-4ef5-44d5-8bbe-422d6bdcc89d",
   "metadata": {},
   "source": [
    "This is not the only feature transform which would work with our dataset.\n",
    "There also exist datasets which would need a much higher dimensional transform.\n",
    "The **kernel trick** permits a comparison in the transform space without needing to explicitly compute the transformation.\n",
    "\n",
    "In the above example, our transform is defined as\n",
    "\n",
    "$$\n",
    "\\phi(\\mathbf{x}) = (x_1^2, x_2^2, \\sqrt{2}x_1x_2).\n",
    "$$\n",
    "\n",
    "This corresponds to the kernel\n",
    "\n",
    "\\begin{align*}\n",
    "k(\\mathbf{x}, \\mathbf{x}') &= (\\mathbf{x}^T\\mathbf{x}')^2\\\\\n",
    "&= (x_1x'_1 + x_2x'_2)^2\\\\\n",
    "&= 2x_1x'_1x_2x'_2 + (x_1x'_1)^2 + (x_2x'_2)^2\\\\\n",
    "&= \\phi(\\mathbf{x})^T \\phi(\\mathbf{x}')\n",
    "\\end{align*}\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "\\phi(\\mathbf{x}) =\n",
    "\\begin{bmatrix}\n",
    "\\sqrt{2}x_1x_2\\\\\n",
    "x_1^2\\\\\n",
    "x_2^2\n",
    "\\end{bmatrix}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f9f00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import ListedColormap\n",
    "from mlxtend.plotting.decision_regions import plot_decision_regions\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "class KernelRegression():\n",
    "    def __init__(self):\n",
    "        self.clf = LogisticRegression()\n",
    "\n",
    "    def predict(self, x):\n",
    "        x = feature_transform(x)\n",
    "        x = self.phi @ x.T\n",
    "        return self.clf.predict(x.T)\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        X = feature_transform(X)\n",
    "        K = X @ X.T\n",
    "        self.phi = X\n",
    "        self.clf.fit(K, Y)\n",
    "\n",
    "model = KernelRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = plot_decision_regions(X_train, y_train, model)\n",
    "fig.add_subplot(ax)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cse6363",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
