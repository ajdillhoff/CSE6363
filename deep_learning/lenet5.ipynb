{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7c6ba42-f5d2-4044-a80b-ded44bdad186",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks\n",
    "\n",
    "In this notebook, we will implement one of the first successful CNNs for handwritten digit recognition and adapt it to work with CIFAR10 data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1ed0cd82-86a4-4d3c-9b85-a0825aeb2b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "from tqdm.notebook import trange, tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06bd8cef-0b41-4928-993a-b0b00a44768f",
   "metadata": {},
   "source": [
    "## LeNet5\n",
    "\n",
    "The original network was meant for $28 \\times 28$ single-channel images.\n",
    "Our data is $32 \\times 32 \\times 3$. The first change is to update the number of input channels in the first layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2829e18-f696-403b-a595-ad88d9fd82bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet5(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet5, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 6, 5), # nn.Conv2d(1, 6, 5) -> nn.Conv2d(3, 6, 5)\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(6, 16, 5),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        \n",
    "        self.estimator = nn.Sequential(\n",
    "            nn.Linear(400, 120), # nn.Linear(256, 120) -> nn.Linear(400, 120)\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(120, 84),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(84, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.shape[0], -1)\n",
    "\n",
    "        return self.estimator(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d610620f-7500-43c6-9d26-e2ece31a4f1f",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8bf4afe4-247c-4fe7-8d46-28ad0298c819",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show(imgs):\n",
    "    if not isinstance(imgs, list):\n",
    "        imgs = [imgs]\n",
    "    fix, axs = plt.subplots(ncols=len(imgs), squeeze=False)\n",
    "    for i, img in enumerate(imgs):\n",
    "        img = img.detach()\n",
    "        img = transforms.functional.to_pil_image(img)\n",
    "        axs[0, i].imshow(np.asarray(img))\n",
    "        axs[0, i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])\n",
    "        \n",
    "        \n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
    "    maxk = max(topk)\n",
    "    batch_size = target.size(0)\n",
    "\n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].view(-1).float().sum(0)\n",
    "        res.append(correct_k.mul_(100.0 / batch_size))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfeb7a1e-8d39-4ef4-b4a7-94f834a16ad0",
   "metadata": {},
   "source": [
    "## Training and Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3499b30c-66b3-4319-98c5-46601090edb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer, logger=None):\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    pbar = tqdm(enumerate(dataloader), total=len(dataloader))\n",
    "    for i, (input, target) in pbar:\n",
    "        \n",
    "        input = input.cuda()\n",
    "        target = target.cuda()\n",
    "\n",
    "        output = model(input)\n",
    "        loss = loss_fn(output, target)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        output = output.float()\n",
    "        loss = loss.float()\n",
    "\n",
    "        prec = accuracy(output.data, target)[0]\n",
    "        losses.update(loss.item(), input.shape[0])\n",
    "        top1.update(prec.item(), input.shape[0])\n",
    "        \n",
    "        if i % print_frequency == 0:\n",
    "            pbar.set_description(\"Epoch [%d]\\t Loss %.2f\\t Prec@1 %.3f (%.3f)\" % (epoch, losses.avg, top1.val, top1.avg))\n",
    "            if logger:\n",
    "                logger.add_scalar(\"training loss\",\n",
    "                                  loss.item(),\n",
    "                                  epoch * len(dataloader) + i)\n",
    "           \n",
    "        \n",
    "def val_loop(dataloader, model, loss_fn, logger=None):\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    pbar = tqdm(enumerate(dataloader), total=len(dataloader))\n",
    "    for i, (input, target) in pbar:\n",
    "        \n",
    "        input = input.cuda()\n",
    "        target = target.cuda()\n",
    "\n",
    "        output = model(input)\n",
    "        loss = loss_fn(output, target)\n",
    "\n",
    "        output = output.float()\n",
    "        loss = loss.float()\n",
    "        \n",
    "        prec = accuracy(output.data, target)[0]\n",
    "        losses.update(loss.item(), input.shape[0])\n",
    "        top1.update(prec.item(), input.shape[0])\n",
    "\n",
    "        if i % print_frequency == 0:\n",
    "            pbar.set_description(\"Epoch [%d]\\t Loss %.2f\\t Prec@1 %.3f (%.3f)\" % (epoch, losses.avg, top1.val, top1.avg))\n",
    "            if logger:\n",
    "                logger.add_scalar(\"validation loss\",\n",
    "                                  loss.item(),\n",
    "                                  epoch * len(dataloader) + i)\n",
    "    \n",
    "    if logger:\n",
    "        logger.add_scalar(\"validation accuracy\",\n",
    "                          top1.avg,\n",
    "                          epoch)\n",
    "            \n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    pbar = tqdm(enumerate(dataloader), total=len(dataloader))\n",
    "    for i, (input, target) in pbar:\n",
    "        \n",
    "        input = input.cuda()\n",
    "        target = target.cuda()\n",
    "\n",
    "        output = model(input)\n",
    "        loss = loss_fn(output, target)\n",
    "\n",
    "        output = output.float()\n",
    "        loss = loss.float()\n",
    "\n",
    "        prec = accuracy(output.data, target)[0]\n",
    "        losses.update(loss.item(), input.shape[0])\n",
    "        top1.update(prec.item(), input.shape[0])\n",
    "\n",
    "    # Print result\n",
    "    print(f\"Average Loss: {losses.avg:>8f}\\nAccuracy: {top1.avg}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5fcabbe-c41d-4fca-83c9-a19a61838a40",
   "metadata": {},
   "source": [
    "# Model Hyperparameters and Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "357b811a-bde8-4f1c-891a-cbf2a9b78829",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Parameters\n",
    "batch_size = 256\n",
    "learning_rate = 1e-3\n",
    "epochs = 20\n",
    "print_frequency = 100\n",
    "\n",
    "model = LeNet5()\n",
    "model.cuda()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bcc4036-360b-4c7f-921e-5a9752e929f0",
   "metadata": {},
   "source": [
    "## Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ffa2ce2-6b06-4d24-baea-bf972b40048c",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = \"./Data/CIFAR10\"\n",
    "\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "\n",
    "dataset = torchvision.datasets.CIFAR10(root_path, train=True, transform=transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    normalize\n",
    "]), download=True)\n",
    "\n",
    "dataset_size = len(dataset)\n",
    "train_size = int(dataset_size * .95)\n",
    "val_size = dataset_size - train_size\n",
    "\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size, shuffle=True,\n",
    "    num_workers=12, pin_memory=False)\n",
    "\n",
    "\n",
    "val_dataloader = torch.utils.data.DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size, shuffle=False,\n",
    "    num_workers=12, pin_memory=False)\n",
    "\n",
    "\n",
    "test_dataloader = torch.utils.data.DataLoader(\n",
    "    torchvision.datasets.CIFAR10(root_path, train=False, transform=transforms.Compose([                                                                     \n",
    "        transforms.ToTensor(),\n",
    "        normalize\n",
    "    ])),\n",
    "    batch_size=batch_size, shuffle=False,\n",
    "    num_workers=12, pin_memory=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ad41b7-53aa-4034-a925-96815af9e1a3",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8bda7f-a37a-45c9-aa65-acc3e99828e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = SummaryWriter(\"runs/lenet5_basic\")\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_loop(train_dataloader, model, criterion, optimizer, logger)\n",
    "    val_loop(val_dataloader, model, criterion, logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262941a4-0c79-4c11-9ddd-f6309f61e916",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loop(test_dataloader, model, criterion)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
