{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REINFORCE\n",
    "\n",
    "This notebook implements and demonstrates the REINFORCE algorithm for the Frozen Lake environment. We will also compare it with REINFORCE with baseline.\n",
    "\n",
    "REINFORCE is an on-policy algorithm for learning a policy $\\pi_\\theta$ to maximize the expected return $J(\\theta)$:\n",
    "\n",
    "$$J(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta} \\left[ \\sum_{t=0}^T \\gamma^t r(s_t, a_t) \\right]$$\n",
    "\n",
    "where $\\tau = (s_0, a_0, s_1, a_1, \\ldots)$ is a trajectory, $r(s_t, a_t)$ is the reward at time $t$, and $\\gamma$ is the discount factor.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class Policy(nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(Policy, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size, 64)\n",
    "        self.fc2 = nn.Linear(64, action_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return torch.softmax(x, dim=1)\n",
    "\n",
    "    def act(self, state):\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0)\n",
    "        probs = self.forward(state)\n",
    "        action = torch.multinomial(probs, 1).item()\n",
    "        log_prob = torch.log(probs.squeeze(0)[action])\n",
    "\n",
    "        return action, log_prob\n",
    "\n",
    "def reinforce(env, policy, optimizer, n_episodes=2000, gamma=0.99):\n",
    "    all_rewards = []\n",
    "    for episode in range(n_episodes):\n",
    "        log_probs = []\n",
    "        rewards = []\n",
    "        state = env.reset()\n",
    "\n",
    "        state = state[0]\n",
    "\n",
    "        while True:\n",
    "            action, log_prob = policy.act(state)\n",
    "            log_probs.append(log_prob)\n",
    "            state, reward, terminated, truncated, _ = env.step(action)\n",
    "            rewards.append(reward)\n",
    "\n",
    "            if terminated or truncated:\n",
    "                break\n",
    "\n",
    "        # Compute cumulative rewards\n",
    "        all_rewards.append(sum(rewards))\n",
    "\n",
    "        returns = []\n",
    "        R = 0\n",
    "        for r in rewards[::-1]:\n",
    "            R = r + gamma * R\n",
    "            returns.insert(0, R)\n",
    "\n",
    "        returns = torch.tensor(returns)\n",
    "        returns = (returns - returns.mean()) / (returns.std() + 1e-9)\n",
    "\n",
    "        policy_loss = []\n",
    "        for log_prob, R in zip(log_probs, returns):\n",
    "            policy_loss.append(-log_prob * R)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        policy_loss = torch.stack(policy_loss).sum()\n",
    "        policy_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if episode % 100 == 0:\n",
    "            print(f\"Episode: {episode}, Total Reward: {sum(rewards)}\")\n",
    "\n",
    "    # Visualize rewards over time\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.plot(all_rewards)\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Cumulative Reward')\n",
    "    plt.title('Policy Gradient Training')\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "    return all_rewards\n",
    "\n",
    "env = gym.make(\"LunarLander-v3\")\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "policy = Policy(state_size, action_size)\n",
    "optimizer = optim.Adam(policy.parameters(), lr=1e-2)\n",
    "r_rewards = reinforce(env, policy, optimizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"LunarLander-v3\", render_mode=\"human\")\n",
    "\n",
    "# Evaluate the policy\n",
    "state = env.reset()\n",
    "state = state[0]\n",
    "rewards = []\n",
    "while True:\n",
    "    # env.render()\n",
    "    action, _ = policy.act(state)\n",
    "    state, reward, terminated, truncated, _ = env.step(action)\n",
    "    rewards.append(reward)\n",
    "    if terminated or truncated:\n",
    "        break\n",
    "\n",
    "env.close()\n",
    "print(\"Total reward:\", sum(rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Baseline(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(Baseline, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 64)\n",
    "        self.fc2 = nn.Linear(64, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = torch.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "\n",
    "def reinforce_with_baseline(env, policy, baseline, policy_optimizer, baseline_optimizer, num_episodes, gamma=0.99):\n",
    "    all_rewards = []\n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        log_probs = []\n",
    "        rewards = []\n",
    "        baselines = []\n",
    "\n",
    "        state = state[0]\n",
    "\n",
    "        while True:\n",
    "            state_tensor = torch.from_numpy(state).float().unsqueeze(0)\n",
    "            action_probs = policy(state_tensor)\n",
    "            action = torch.multinomial(action_probs, 1).item()\n",
    "\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "\n",
    "            log_prob = torch.log(action_probs.squeeze(0)[action])\n",
    "            log_probs.append(log_prob)\n",
    "            rewards.append(reward)\n",
    "\n",
    "            baseline_value = baseline(state_tensor)\n",
    "            baselines.append(baseline_value)\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "            if terminated or truncated:\n",
    "                break\n",
    "\n",
    "        all_rewards.append(sum(rewards))\n",
    "        returns = []\n",
    "        G = 0\n",
    "        for r in reversed(rewards):\n",
    "            G = r + gamma * G\n",
    "            returns.insert(0, G)\n",
    "\n",
    "        returns = torch.tensor(returns, dtype=torch.float)\n",
    "        returns = (returns - returns.mean()) / (returns.std() + 1e-9)\n",
    "        baselines = torch.cat(baselines).squeeze()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            advantages = returns - baselines\n",
    "\n",
    "        policy_loss = []\n",
    "        for log_prob, advantage in zip(log_probs, advantages):\n",
    "            policy_loss.append(-log_prob * advantage)\n",
    "\n",
    "        policy_optimizer.zero_grad()\n",
    "        policy_loss = torch.stack(policy_loss).mean()\n",
    "        policy_loss.backward()\n",
    "        policy_optimizer.step()\n",
    "\n",
    "        baseline_loss = nn.MSELoss()(baselines, returns)\n",
    "        baseline_optimizer.zero_grad()\n",
    "        baseline_loss.backward()\n",
    "        baseline_optimizer.step()\n",
    "\n",
    "        if episode % 100 == 0:\n",
    "            print(f\"Episode {episode}, Policy Loss: {policy_loss.item()}, Baseline Loss: {baseline_loss.item()}\")\n",
    "\n",
    "    # Visualize rewards over time\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.plot(all_rewards)\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Cumulative Reward')\n",
    "    plt.title('Policy Gradient Training')\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "    return all_rewards\n",
    "\n",
    "env = gym.make(\"LunarLander-v3\")\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "\n",
    "policy = Policy(state_size, action_size)\n",
    "baseline = Baseline(state_size, 1)\n",
    "policy_optimizer = optim.Adam(policy.parameters(), lr=1e-2)\n",
    "baseline_optimizer = optim.Adam(baseline.parameters(), lr=1e-2)\n",
    "\n",
    "num_episodes = 2000\n",
    "rb_rewards = reinforce_with_baseline(env, policy, baseline, policy_optimizer, baseline_optimizer, num_episodes, 0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Smooth rewards and plot both reinforce and reinforce with baseline\n",
    "import matplotlib.pyplot as plt\n",
    "def smooth_rewards(rewards, window_size=100):\n",
    "    smoothed = []\n",
    "    for i in range(len(rewards)):\n",
    "        if i < window_size:\n",
    "            smoothed.append(np.mean(rewards[:i+1]))\n",
    "        else:\n",
    "            smoothed.append(np.mean(rewards[i-window_size+1:i+1]))\n",
    "    return smoothed\n",
    "\n",
    "smoothed_r_rewards = smooth_rewards(r_rewards)\n",
    "smoothed_rb_rewards = smooth_rewards(rb_rewards)\n",
    "\n",
    "plt.plot(smoothed_r_rewards, label='REINFORCE')\n",
    "plt.plot(smoothed_rb_rewards, label='REINFORCE with Baseline')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Cumulative Reward')\n",
    "plt.title('Comparison of REINFORCE and REINFORCE with Baseline')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"LunarLander-v3\", render_mode=\"human\")\n",
    "\n",
    "# Evaluate the policy\n",
    "state = env.reset()\n",
    "state = state[0]\n",
    "rewards = []\n",
    "while True:\n",
    "    # env.render()\n",
    "    action, _ = policy.act(state)\n",
    "    state, reward, terminated, truncated, _ = env.step(action)\n",
    "    rewards.append(reward)\n",
    "    if terminated or truncated:\n",
    "        break\n",
    "\n",
    "env.close()\n",
    "print(\"Total reward:\", sum(rewards))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cse6363",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
